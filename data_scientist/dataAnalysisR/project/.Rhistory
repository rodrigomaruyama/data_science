setwd("D://Documents/ead/Udacity//data_scientist/dataAnalysisR/project/")
setwd("D://Documents/ead/Udacity/data_scientist/dataAnalysisR/project/")
setwd("D://Documents/ead/Udacity/dsNanodegree/data_scientist/dataAnalysisR/project/")
library(ROCR)
library(randomForest)
# Load the Data
wdf <- read.csv('data/wineQualityWhites.csv')
# Find the outliers based in the 1.5*IQR value
outliers <- data.frame()
for (f in names(wdf[2, -12])) {
q1 <- as.numeric(quantile(wdf[, f], 0.25))
q3 <- as.numeric(quantile(wdf[, f], 0.75))
step <- 1.5*(q3 - q1)
outliers <- rbind(outliers,
subset(wdf, (wdf[, f] <= (q1 - step)) | (wdf[, f] >= (q3 + step))))
}
# Just getting the outliers in more than one feature
outliers <- outliers[duplicated(outliers), ]
dim(outliers)
# Removing the outliers from the dataset
wdf.outliers <- anti_join(wdf, outliers)
wdf.outliers$X <- NULL
# Find the outliers based in the 1.5*IQR value
outliers <- data.frame()
for (f in names(wdf[2, -12])) {
q1 <- as.numeric(quantile(wdf[, f], 0.25))
q3 <- as.numeric(quantile(wdf[, f], 0.75))
step <- 1.5*(q3 - q1)
outliers <- rbind(outliers,
subset(wdf, (wdf[, f] <= (q1 - step)) | (wdf[, f] >= (q3 + step))))
}
# Just getting the outliers in more than one feature
outliers <- outliers[duplicated(outliers), ]
dim(outliers)
# Removing the outliers from the dataset
wdf.outliers <- anti_join(wdf, outliers)
wdf.outliers$X <- NULL
library(dplyr)
# Removing the outliers from the dataset
wdf.outliers <- anti_join(wdf, outliers)
wdf.outliers$quality <- as.factor(wdf.outliers$quality)
# Creating Train.outliers and Test.outliers datasets
set.seed(13)
samp <- sample(nrow(wdf.outliers), 0.4 * nrow(wdf.outliers))
train.outliers <- wdf.outliers[samp, ]
test.outliers <- wdf.outliers[-samp, ]
# Random Forest Raw
model <- randomForest(quality ~ . - quality, data = train.outliers)
rf.pred <- predict(model, newdata = test.outliers[, -12])
rf.pred <- predict(model, newdata = test.outliers[, -12])
View(train.outliers)
# Load the Data
wdf <- read.csv('data/wineQualityWhites.csv')
# Find the outliers based in the 1.5*IQR value
outliers <- data.frame()
for (f in names(wdf[2, -12])) {
q1 <- as.numeric(quantile(wdf[, f], 0.25))
q3 <- as.numeric(quantile(wdf[, f], 0.75))
step <- 1.5*(q3 - q1)
outliers <- rbind(outliers,
subset(wdf, (wdf[, f] <= (q1 - step)) | (wdf[, f] >= (q3 + step))))
}
# Just getting the outliers in more than one feature
outliers <- outliers[duplicated(outliers), ]
dim(outliers)
# Removing the outliers from the dataset
wdf.outliers <- anti_join(wdf, outliers)
wdf.outliers$X <- NULL
wdf.outliers$quality <- as.factor(wdf.outliers$quality)
# Creating Train.outliers and Test.outliers datasets
set.seed(13)
samp <- sample(nrow(wdf.outliers), 0.4 * nrow(wdf.outliers))
train.outliers <- wdf.outliers[samp, ]
test.outliers <- wdf.outliers[-samp, ]
# Random Forest Raw
model <- randomForest(quality ~ . - quality, data = train.outliers)
View(train.outliers)
rf.pred <- predict(model, newdata = test.outliers[, -quality])
rf.pred <- predict(model, newdata = test.outliers[, -12])
rf.pred.prob <- predict(model, newdata = test.outliers[, -12], type = 'prob')
rf.pred <- as.numeric(rf.pred)
test.outliers$quality <- as.numeric(test.outliers$quality)
s <- length(rf.pred)
a <- 0
bin.pred <- c(1:2819)
while (a < s) {
a <- a + 1
if (rf.pred[a] == test.outliers$quality[a]) {
bin.pred[a] <- 1
print(paste('TRUE', a, rf.pred[a], test.outliers$quality[a], sep = '--'))
} else {
bin.pred[a] <- 0
print(paste('FALSE', a, rf.pred[a], test.outliers$quality[a], sep = '--'))
}
}
pred <- prediction(rf.pred, test.outliers$quality)
dim(rf.pred)
length(rf.pred)
length(test.outliers$quality)
#
pred <- prediction(rf.pred.prob, bin.pred)
dim(rf.pred)
length(rf.pred.prob)
length(bin.pred)
rf.pred.prob <- predict(model, newdata = test.outliers[, -12], type = 'response')
length(rf.pred.prob)
#
pred <- prediction(rf.pred.prob, bin.pred)
rf.pred.prob <- predict(model, newdata = test.outliers[, -12], type = 'prob')
View(rf.pred.prob)
